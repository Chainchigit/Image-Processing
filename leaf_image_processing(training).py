# -*- coding: utf-8 -*-
"""Leaf image processing(Training)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kyGsigPq1BzBg7Ihze3PUOBqIXnFLXMz

# Model Training
"""

import tensorflow as tf
print("TF version:", tf.__version__)
print("GPU:", tf.config.list_physical_devices('GPU'))

from google.colab import drive
drive.mount('/content/drive')

import os

DATA_DIR = "/content/drive/MyDrive/300"

classes = ["good", "hole", "low", "oxy", "reject"]
for c in classes:
    p = os.path.join(DATA_DIR, c)
    print(c, "=>", len(os.listdir(p)))

import tensorflow as tf

IMG_SIZE = (128, 128)
BATCH_SIZE = 32
SEED = 42

train_ds = tf.keras.utils.image_dataset_from_directory(
    DATA_DIR,
    labels="inferred",
    label_mode="categorical",
    validation_split=0.2,
    subset="training",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=True
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    DATA_DIR,
    labels="inferred",
    label_mode="categorical",
    validation_split=0.2,
    subset="validation",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=False
)

class_names = train_ds.class_names
print("class order:", class_names)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)
val_ds = val_ds.cache().prefetch(AUTOTUNE)

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.08),
    tf.keras.layers.RandomZoom(0.15),
    tf.keras.layers.RandomContrast(0.1),
], name="data_augmentation")

from tensorflow.keras import layers, models

base_model = tf.keras.applications.EfficientNetB0(
    include_top=False,
    weights="imagenet",
    input_shape=(128, 128, 3)
)
base_model.trainable = False

inputs = layers.Input(shape=(128, 128, 3))
x = data_augmentation(inputs)
x = tf.keras.applications.efficientnet.preprocess_input(x)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(len(class_names), activation="softmax")(x)

model = models.Model(inputs, outputs)

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
    tf.keras.callbacks.ModelCheckpoint("best_leaf_model.h5", save_best_only=True)
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=12,
    callbacks=callbacks
)



base_model.trainable = True
for layer in base_model.layers[:200]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

history_ft = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=6,
    callbacks=callbacks
)

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

# y_true = 0..K-1 จาก val_ds
y_true = np.concatenate([np.argmax(y.numpy(), axis=1) for x, y in val_ds], axis=0)

# y_pred = 0..K-1
y_pred = np.argmax(model.predict(val_ds), axis=1)

labels = list(range(len(class_names)))  # [0,1,2,3]

print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred, labels=labels))
print("\nReport:\n", classification_report(
    y_true, y_pred,
    labels=labels,
    target_names=class_names,
    zero_division=0
))

import os
classes = ["good","hole","low","oxy","reject"]
for c in classes:
    p = os.path.join(DATA_DIR, c)
    files = [f for f in os.listdir(p) if f.lower().endswith((".jpg",".jpeg",".png"))]
    print(c, "=", len(files))

import numpy as np

y_true = np.concatenate([np.argmax(y.numpy(), axis=1) for x, y in val_ds], axis=0)
present = np.unique(y_true)

print("val has class indices:", present)
print("val has class names:", [class_names[i] for i in present])
print("val counts per class:")
for i, name in enumerate(class_names):
    print(name, "=", int(np.sum(y_true == i)))

import os, random, shutil

SRC = DATA_DIR
OUT = "/content/drive/MyDrive/leaf_split"
VAL_RATIO = 0.25
classes = ["good","hole","low","oxy","reject"]

random.seed(42)

# สร้างโฟลเดอร์
for split in ["train","val"]:
    for c in classes:
        os.makedirs(os.path.join(OUT, split, c), exist_ok=True)

# ล้างของเก่า (กันไฟล์ซ้ำ) - ระวังถ้า OUT มีไฟล์สำคัญ
for split in ["train","val"]:
    for c in classes:
        folder = os.path.join(OUT, split, c)
        for f in os.listdir(folder):
            os.remove(os.path.join(folder, f))

# แบ่งไฟล์แบบรับประกันทุกคลาสมี val
for c in classes:
    files = [f for f in os.listdir(os.path.join(SRC, c)) if f.lower().endswith((".jpg",".jpeg",".png"))]
    random.shuffle(files)

    n_val = max(1, int(len(files)*VAL_RATIO))  # อย่างน้อย 1 รูปต่อคลาส
    val_files = files[:n_val]
    train_files = files[n_val:]

    for f in train_files:
        shutil.copy2(os.path.join(SRC, c, f), os.path.join(OUT, "train", c, f))
    for f in val_files:
        shutil.copy2(os.path.join(SRC, c, f), os.path.join(OUT, "val", c, f))

print("split done at:", OUT)

import tensorflow as tf

train_dir = "/content/drive/MyDrive/leaf_split/train"
val_dir   = "/content/drive/MyDrive/leaf_split/val"

train_ds = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    image_size=(128,128),
    batch_size=32,
    label_mode="categorical",
    shuffle=True
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    val_dir,
    image_size=(128,128),
    batch_size=32,
    label_mode="categorical",
    shuffle=False
)

class_names = train_ds.class_names
print("class order:", class_names)

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

# 1) y_true: ดึง label จาก val_ds
y_true = np.concatenate([np.argmax(y.numpy(), axis=1) for _, y in val_ds], axis=0)

# 2) y_pred: predict ทีละ batch แล้วรวม
y_pred_prob = model.predict(val_ds, verbose=0)
y_pred = np.argmax(y_pred_prob, axis=1)

# 3) บังคับ labels ให้ครบ 4 คลาสเสมอ
labels = list(range(len(class_names)))

print("val counts per class:")
for i, name in enumerate(class_names):
    print(name, int(np.sum(y_true == i)))

print("\nConfusion Matrix (rows=true, cols=pred):")
print(confusion_matrix(y_true, y_pred, labels=labels))

print("\nReport:")
print(classification_report(
    y_true, y_pred,
    labels=labels,
    target_names=class_names,
    zero_division=0
))

import os, json

OUT_DIR = "/content/drive/MyDrive/leaf_model_export"
os.makedirs(OUT_DIR, exist_ok=True)

model.save(os.path.join(OUT_DIR, "leaf_classifier.h5"))

with open(os.path.join(OUT_DIR, "class_names.json"), "w") as f:
    json.dump(class_names, f)

print("Exported to:", OUT_DIR)